{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ade4b9",
   "metadata": {},
   "source": [
    "\n",
    "# WhaRAGBot — RAG local con tus chats de WhatsApp\n",
    "\n",
    "Cuaderno paso a paso para: (1) leer los ZIP exportados de WhatsApp, (2) generar pares contexto→respuesta con tus mensajes, (3) indexar con embeddings locales y FAISS, y (4) chatear usando OpenAI API con contexto recuperado por RAG.\n",
    "\n",
    "> ⚠️ Este repo está pensado para ser público. **No subas tus chats ni datos procesados**: quedan ignorados en `.gitignore`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b4bc6",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Entorno (solo kernel)\n",
    "Crea o reutiliza el entorno virtual y registra el kernel. Las dependencias del proyecto se instalan en el paso 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejecuta una vez; crea .venv con Python 3.12 y registra el kernel.\n",
    "# Las dependencias del proyecto se instalan en el paso 1 (Dependencias).\n",
    "import os, subprocess, pathlib, sys\n",
    "from subprocess import CalledProcessError\n",
    "ENV_DIR = pathlib.Path(\"..\").resolve() / \".venv\"\n",
    "PY_BIN = os.getenv(\"PYTHON_BIN\", \"python3.12\")\n",
    "KERNEL_NAME = \"wha-ragbot\"\n",
    "DISPLAY_NAME = \"wha-ragbot (py312)\"\n",
    "\n",
    "\n",
    "def create_with_venv():\n",
    "    subprocess.check_call([PY_BIN, \"-m\", \"venv\", str(ENV_DIR)])\n",
    "\n",
    "\n",
    "def create_with_virtualenv():\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"virtualenv\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"virtualenv\", \"-p\", PY_BIN, str(ENV_DIR)])\n",
    "\n",
    "if ENV_DIR.exists():\n",
    "    print(f\"Ya existe {ENV_DIR}\")\n",
    "else:\n",
    "    try:\n",
    "        create_with_venv()\n",
    "        print(f\"Creado entorno con {PY_BIN} en {ENV_DIR} (venv)\")\n",
    "    except CalledProcessError as e:\n",
    "        print(f\"No se pudo crear con venv ({e}); probando con virtualenv...\")\n",
    "        create_with_virtualenv()\n",
    "        print(f\"Creado entorno con {PY_BIN} en {ENV_DIR} (virtualenv)\")\n",
    "\n",
    "# comprobar versión\n",
    "try:\n",
    "    out = subprocess.check_output([str(ENV_DIR/\"bin/python\"), \"-c\", \"import sys;print(sys.version.split()[0])\"], text=True).strip()\n",
    "    if not out.startswith(\"3.12\"):\n",
    "        print(f\"⚠️ El venv usa Python {out}; borra .venv y reejecuta esta celda para crearlo con 3.12\")\n",
    "    else:\n",
    "        print(f\"Usando venv con Python {out}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ No se pudo comprobar la versión del venv: {e}\")\n",
    "\n",
    "# instalar solo ipykernel para registrar el kernel\n",
    "subprocess.check_call([str(ENV_DIR/\"bin/python\"), \"-m\", \"pip\", \"install\", \"-q\", \"ipykernel\"])\n",
    "subprocess.check_call([str(ENV_DIR/\"bin/python\"), \"-m\", \"ipykernel\", \"install\", \"--user\", \"--name\", KERNEL_NAME, \"--display-name\", DISPLAY_NAME])\n",
    "print(f\"Kernel registrado como '{DISPLAY_NAME}'. Ahora selecciónalo en VS Code/Jupyter y reinicia el kernel.\")\n",
    "print(\"Luego pasa al paso 1 para instalar requirements.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099a3dd",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Dependencias (ejecuta una vez)\n",
    "Si ya instalaste los requisitos, puedes saltar esta celda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r ../requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399243ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e559e4",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Configuración básica\n",
    "Ajusta rutas y parámetros principales. `CHATS_ZIP_DIR` debe apuntar a la carpeta que contiene los `.zip` exportados por WhatsApp (opción \"sin archivos multimedia\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, zipfile, re, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "CHATS_ZIP_DIR = Path(os.getenv(\"CHATS_ZIP_DIR\", str(PROJECT_ROOT / \"Chats en .zip\")))\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "INDEX_DIR = PROJECT_ROOT / \"index\"\n",
    "MY_NAME = os.getenv(\"MY_NAME\", \"Tu Nombre\")\n",
    "CTX_WINDOW = 4\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"intfloat/multilingual-e5-small\")\n",
    "GEN_MODEL = os.getenv(\"GEN_MODEL\", \"gpt-4.1-mini\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac388b92",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Funciones de parseo\n",
    "Soporta exportes típicos de Android/iPhone en español. Filtra mensajes de sistema y mantiene multilinea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5cb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "START_PATTERNS = [\n",
    "    re.compile(r\"^(?P<date>\\d{1,2}/\\d{1,2}/\\d{2,4}), (?P<time>\\d{1,2}:\\d{2}) - (?P<sender>[^:]+): (?P<text>.*)$\"),\n",
    "    re.compile(r\"^\\[(?P<date>\\d{1,2}/\\d{1,2}/\\d{2,4}), (?P<time>\\d{1,2}:\\d{2}(?:\\s?[ap]\\.?m\\.?))\\] (?P<sender>[^:]+): (?P<text>.*)$\"),\n",
    "]\n",
    "\n",
    "SYSTEM_SNIPPETS = [\n",
    "    \"cifrado de extremo a extremo\",\n",
    "    \"cambió el asunto\",\n",
    "    \"cambió la foto\",\n",
    "    \"cambió la descripción\",\n",
    "    \"creó este grupo\",\n",
    "    \"te añadieron\",\n",
    "    \"mensaje eliminado\",\n",
    "    \"multimedia omitido\",\n",
    "]\n",
    "\n",
    "\n",
    "def match_start(line: str):\n",
    "    for pat in START_PATTERNS:\n",
    "        m = pat.match(line)\n",
    "        if m:\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_chat_text(text: str, chat_name: str):\n",
    "    rows = []\n",
    "    current = None\n",
    "    for raw_line in text.splitlines():\n",
    "        line = raw_line.strip(\"﻿\")  # quitar BOM si existe\n",
    "        m = match_start(line)\n",
    "        if m:\n",
    "            if current:\n",
    "                rows.append(current)\n",
    "            current = {\n",
    "                \"date\": m.group(\"date\"),\n",
    "                \"time\": m.group(\"time\"),\n",
    "                \"sender\": m.group(\"sender\").strip(),\n",
    "                \"text\": m.group(\"text\").strip(),\n",
    "                \"chat_name\": chat_name,\n",
    "            }\n",
    "        else:\n",
    "            if current:\n",
    "                current[\"text\"] += \" \" + line.strip()\n",
    "    if current:\n",
    "        rows.append(current)\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"date\"] + \" \" + df[\"time\"], dayfirst=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"timestamp\"])\n",
    "    # filtrar mensajes de sistema\n",
    "    mask = df[\"text\"].str.lower().apply(lambda t: not any(snippet in t.lower() for snippet in SYSTEM_SNIPPETS))\n",
    "    df = df[mask]\n",
    "    df[\"text\"] = df[\"text\"].str.replace(' ', ' ', regex=False)\n",
    "    return df[[\"timestamp\", \"sender\", \"text\", \"chat_name\"]]\n",
    "\n",
    "\n",
    "def parse_zip_chat(zip_path: Path):\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        txt_files = [n for n in zf.namelist() if n.lower().endswith(\".txt\")]\n",
    "        if not txt_files:\n",
    "            print(f\"⚠️ No se encontró .txt en {zip_path}\")\n",
    "            return pd.DataFrame()\n",
    "        raw = zf.read(txt_files[0])\n",
    "        for encoding in (\"utf-8\", \"latin-1\"):\n",
    "            try:\n",
    "                text = raw.decode(encoding)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        else:\n",
    "            raise UnicodeDecodeError(\"No se pudo decodificar el chat\")\n",
    "        return parse_chat_text(text, chat_name=zip_path.stem)\n",
    "\n",
    "\n",
    "def parse_csv_chat(csv_path: Path, my_name: str):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ No se pudo leer CSV {csv_path.name}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"timestamp\", \"sender\", \"text\", \"chat_name\"])\n",
    "\n",
    "    required = {\"question\", \"wenceslao_answer\"}\n",
    "    if not required.issubset(set(df.columns)):\n",
    "        print(f\"⚠️ CSV {csv_path.name} no tiene columnas requeridas {required}\")\n",
    "        return pd.DataFrame(columns=[\"timestamp\", \"sender\", \"text\", \"chat_name\"])\n",
    "\n",
    "    contact_col = \"contact\" if \"contact\" in df.columns else None\n",
    "    qdt_col = \"question_dt\" if \"question_dt\" in df.columns else None\n",
    "    adt_col = \"answer_dt\" if \"answer_dt\" in df.columns else None\n",
    "\n",
    "    rows = []\n",
    "    for i, r in df.iterrows():\n",
    "        contact = str(r.get(contact_col, \"Contacto\")) if contact_col else \"Contacto\"\n",
    "        chat_name = f\"Chat CSV con {contact}\" if contact else f\"Chat CSV {csv_path.stem}\"\n",
    "\n",
    "        qtxt = str(r.get(\"question\", \"\") or \"\").strip()\n",
    "        atxt = str(r.get(\"wenceslao_answer\", \"\") or \"\").strip()\n",
    "        qts = pd.to_datetime(r.get(qdt_col), dayfirst=True, errors=\"coerce\") if qdt_col else pd.NaT\n",
    "        ats = pd.to_datetime(r.get(adt_col), dayfirst=True, errors=\"coerce\") if adt_col else pd.NaT\n",
    "\n",
    "        if qtxt:\n",
    "            rows.append({\n",
    "                \"timestamp\": qts,\n",
    "                \"sender\": contact,\n",
    "                \"text\": qtxt,\n",
    "                \"chat_name\": chat_name,\n",
    "            })\n",
    "        if atxt:\n",
    "            rows.append({\n",
    "                \"timestamp\": ats,\n",
    "                \"sender\": my_name,\n",
    "                \"text\": atxt,\n",
    "                \"chat_name\": chat_name,\n",
    "            })\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"timestamp\", \"sender\", \"text\", \"chat_name\"])\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out = out.dropna(subset=[\"timestamp\"]) \n",
    "    out[\"text\"] = out[\"text\"].astype(str).str.replace(' ', ' ', regex=False)\n",
    "    out = out.sort_values([\"chat_name\", \"timestamp\"]).reset_index(drop=True)\n",
    "    return out[[\"timestamp\", \"sender\", \"text\", \"chat_name\"]]\n",
    "\n",
    "\n",
    "def parse_input_file(path: Path, my_name: str):\n",
    "    low = path.suffix.lower()\n",
    "    if low == \".zip\":\n",
    "        return parse_zip_chat(path)\n",
    "    if low == \".csv\":\n",
    "        return parse_csv_chat(path, my_name=my_name)\n",
    "    return pd.DataFrame(columns=[\"timestamp\", \"sender\", \"text\", \"chat_name\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd868fe",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Cargar todos los ZIP y guardar mensajes procesados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "\n",
    "INGEST_CACHE_DIR = DATA_DIR / \"ingest_cache\"\n",
    "INGEST_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INGEST_MANIFEST_PATH = INGEST_CACHE_DIR / \"manifest.json\"\n",
    "\n",
    "\n",
    "def _load_json(path: Path, default):\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _save_json(path: Path, payload):\n",
    "    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "def _file_sig(path: Path):\n",
    "    st = path.stat()\n",
    "    return {\"size\": int(st.st_size), \"mtime_ns\": int(st.st_mtime_ns)}\n",
    "\n",
    "\n",
    "def _sha256_file(path: Path, chunk_size: int = 1024 * 1024):\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "zip_files = sorted(CHATS_ZIP_DIR.glob(\"*.zip\"))\n",
    "csv_files = sorted(CHATS_ZIP_DIR.glob(\"*.csv\"))\n",
    "all_files = zip_files + csv_files\n",
    "print(f\"Encontrados {len(zip_files)} ZIPs y {len(csv_files)} CSVs en {CHATS_ZIP_DIR}\")\n",
    "print(f\"[Paso 4] Inicio: {pd.Timestamp.now().isoformat()} | archivos a revisar: {len(all_files)}\")\n",
    "\n",
    "ingest_t0 = time.perf_counter()\n",
    "manifest = _load_json(INGEST_MANIFEST_PATH, {\"files\": {}, \"version\": 1})\n",
    "manifest_files = manifest.get(\"files\", {})\n",
    "PARSER_CACHE_VERSION = \"v2_my_name_tag\"\n",
    "parse_tag = f\"{PARSER_CACHE_VERSION}|my_name={str(MY_NAME).strip()}\"\n",
    "frames = []\n",
    "\n",
    "cache_fast_hits = 0\n",
    "cache_hash_hits = 0\n",
    "reparsed = 0\n",
    "failed = 0\n",
    "rows_total = 0\n",
    "\n",
    "for idx, fp in enumerate(tqdm(all_files, desc=\"Paso 4 - ingesta\", unit=\"archivo\"), start=1):\n",
    "    key = str(fp.resolve())\n",
    "    sig = _file_sig(fp)\n",
    "    meta = manifest_files.get(key, {})\n",
    "\n",
    "    cache_file = meta.get(\"cache_file\", \"\")\n",
    "    cache_path = INGEST_CACHE_DIR / cache_file if cache_file else None\n",
    "\n",
    "    fast_hit = (\n",
    "        cache_path is not None\n",
    "        and cache_path.exists()\n",
    "        and meta.get(\"size\") == sig[\"size\"]\n",
    "        and meta.get(\"mtime_ns\") == sig[\"mtime_ns\"]\n",
    "        and meta.get(\"parse_tag\") == parse_tag\n",
    "    )\n",
    "\n",
    "    df = None\n",
    "    file_sha = None\n",
    "\n",
    "    if fast_hit:\n",
    "        try:\n",
    "            df = pd.read_parquet(cache_path)\n",
    "            cache_fast_hits += 1\n",
    "        except Exception:\n",
    "            df = None\n",
    "\n",
    "    if df is None:\n",
    "        file_sha = _sha256_file(fp)\n",
    "        hash_hit = (\n",
    "            cache_path is not None\n",
    "            and cache_path.exists()\n",
    "            and meta.get(\"sha256\") == file_sha\n",
    "            and meta.get(\"parse_tag\") == parse_tag\n",
    "        )\n",
    "\n",
    "        if hash_hit:\n",
    "            try:\n",
    "                df = pd.read_parquet(cache_path)\n",
    "                cache_hash_hits += 1\n",
    "            except Exception:\n",
    "                df = None\n",
    "\n",
    "        if df is None:\n",
    "            try:\n",
    "                df = parse_input_file(fp, my_name=MY_NAME)\n",
    "                reparsed += 1\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                print(f\"[Paso 4][WARN] fallo parseando {fp.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            cache_file = f\"{file_sha}.parquet\"\n",
    "            cache_path = INGEST_CACHE_DIR / cache_file\n",
    "            df.to_parquet(cache_path, index=False)\n",
    "\n",
    "        manifest_files[key] = {\n",
    "            \"sha256\": file_sha,\n",
    "            \"cache_file\": cache_file,\n",
    "            \"rows\": int(len(df)),\n",
    "            \"size\": sig[\"size\"],\n",
    "            \"mtime_ns\": sig[\"mtime_ns\"],\n",
    "            \"parse_tag\": parse_tag,\n",
    "        }\n",
    "\n",
    "    rows_total += int(len(df))\n",
    "    frames.append(df)\n",
    "\n",
    "    if idx % 10 == 0 or idx == len(all_files):\n",
    "        print(\n",
    "            f\"[Paso 4] progreso {idx}/{len(all_files)} | \"\n",
    "            f\"fast={cache_fast_hits} hash={cache_hash_hits} reparsed={reparsed} failed={failed} \"\n",
    "            f\"rows_acum={rows_total}\"\n",
    "        )\n",
    "\n",
    "# Limpia entradas de archivos borrados.\n",
    "valid_keys = {str(p.resolve()) for p in all_files}\n",
    "for old_key in list(manifest_files.keys()):\n",
    "    if old_key not in valid_keys:\n",
    "        manifest_files.pop(old_key, None)\n",
    "\n",
    "manifest[\"files\"] = manifest_files\n",
    "manifest[\"updated_at\"] = pd.Timestamp.now(\"UTC\").isoformat()\n",
    "_save_json(INGEST_MANIFEST_PATH, manifest)\n",
    "\n",
    "messages = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=[\"timestamp\", \"sender\", \"text\", \"chat_name\"])\n",
    "messages = messages.dropna(subset=[\"timestamp\"]).sort_values([\"chat_name\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "elapsed = time.perf_counter() - ingest_t0\n",
    "print(\n",
    "    f\"[Paso 4] Fin en {elapsed:.1f}s | total_filas={len(messages)} | \"\n",
    "    f\"fast={cache_fast_hits} hash={cache_hash_hits} reparsed={reparsed} failed={failed}\"\n",
    ")\n",
    "print(messages.head())\n",
    "\n",
    "try:\n",
    "    messages.to_parquet(DATA_DIR / \"messages.parquet\", index=False)\n",
    "    print(f\"Guardado messages.parquet con {len(messages)} filas en {DATA_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ No se pudo escribir parquet ({e}) -> guardando CSV\")\n",
    "    messages.to_csv(DATA_DIR / \"messages.csv\", index=False)\n",
    "    print(f\"Guardado messages.csv con {len(messages)} filas en {DATA_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371306ba",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 Limpieza y etiquetas (recomendado)\n",
    "Normaliza texto, marca preguntas y bajo contenido. Esto mejora la recuperacion y reduce respuestas raras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc633d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "ACK_WORDS = {\"ok\", \"oki\", \"okey\", \"vale\", \"si\", \"sii\", \"sip\", \"aja\", \"mmm\", \"mm\", \"mhm\", \"jaja\", \"jajaja\", \"jeje\", \"jaj\", \"xd\", \"xD\"}\n",
    "QUESTION_PREFIXES = (\"que \", \"como \", \"por que \", \"por qué \", \"cuando \", \"cuanto \", \"donde \", \"quien \", \"quien \", \"cual \", \"cuales \")\n",
    "SELF_FACT_HINTS = [\"me llamo\", \"mi nombre\", \"me llaman\", \"soy de\", \"vivo en\", \"trabajo\", \"curro\", \"estudio\", \"tengo \"]\n",
    "MY_NAME_PLACEHOLDERS = {\"\", \"tu nombre\", \"your name\", \"mi nombre\"}\n",
    "\n",
    "\n",
    "def _normalize_text(t: str) -> str:\n",
    "    t = str(t or \"\")\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"\\u200b\", \"\").replace(\"\\ufeff\", \"\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def _normalize_sender_name(name: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", str(name or \"\")).strip()\n",
    "\n",
    "\n",
    "def _sender_key(name: str) -> str:\n",
    "    return _normalize_sender_name(name).casefold()\n",
    "\n",
    "\n",
    "def _has_url(t: str) -> bool:\n",
    "    return bool(re.search(r\"(https?://|www\\.)\", t or \"\", flags=re.I))\n",
    "\n",
    "\n",
    "def _is_question(t: str) -> bool:\n",
    "    t = (t or \"\").strip().lower()\n",
    "    if \"?\" in t:\n",
    "        return True\n",
    "    return any(t.startswith(p) for p in QUESTION_PREFIXES)\n",
    "\n",
    "\n",
    "def _is_low_signal(t: str) -> bool:\n",
    "    t = (t or \"\").strip().lower()\n",
    "    if not t:\n",
    "        return True\n",
    "    if t in ACK_WORDS:\n",
    "        return True\n",
    "    if len(t) <= 2:\n",
    "        return True\n",
    "    if re.fullmatch(r\"[\\W_]+\", t):\n",
    "        return True\n",
    "    if sum(ch.isalnum() for ch in t) == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _signal_score(t: str, is_question: bool = False, has_url: bool = False) -> float:\n",
    "    t = (t or \"\").strip()\n",
    "    tokens = len(t.split())\n",
    "    score = min(1.0, tokens / 12)\n",
    "    if len(t) >= 80:\n",
    "        score += 0.10\n",
    "    if is_question:\n",
    "        score += 0.05\n",
    "    if has_url:\n",
    "        score -= 0.10\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "def _is_self_fact(t: str) -> bool:\n",
    "    n = (t or \"\").lower()\n",
    "    return any(h in n for h in SELF_FACT_HINTS)\n",
    "\n",
    "\n",
    "def _resolve_my_name(df: pd.DataFrame, configured_name: str):\n",
    "    cfg_raw = _normalize_sender_name(configured_name)\n",
    "    cfg_key = _sender_key(cfg_raw)\n",
    "\n",
    "    senders = df[\"sender\"].astype(str).map(_normalize_sender_name)\n",
    "    sender_keys = senders.map(_sender_key)\n",
    "\n",
    "    if cfg_key and cfg_key not in MY_NAME_PLACEHOLDERS and (sender_keys == cfg_key).any():\n",
    "        chosen = senders[sender_keys == cfg_key].value_counts().idxmax()\n",
    "        return str(chosen), \"config_match\"\n",
    "\n",
    "    tmp = pd.DataFrame({\n",
    "        \"sender\": senders,\n",
    "        \"sender_key\": sender_keys,\n",
    "        \"chat_name\": df[\"chat_name\"].astype(str),\n",
    "    })\n",
    "    tmp = tmp[tmp[\"sender_key\"] != \"\"]\n",
    "    if tmp.empty:\n",
    "        fallback = cfg_raw or \"Tu Nombre\"\n",
    "        return fallback, \"fallback_default\"\n",
    "\n",
    "    coverage = tmp.groupby(\"sender_key\")[\"chat_name\"].nunique().sort_values(ascending=False)\n",
    "    top_key = str(coverage.index[0])\n",
    "    top_coverage = int(coverage.iloc[0])\n",
    "    total_chats = max(1, int(tmp[\"chat_name\"].nunique()))\n",
    "\n",
    "    if total_chats >= 2 and top_coverage >= max(2, int(total_chats * 0.5)):\n",
    "        chosen = tmp.loc[tmp[\"sender_key\"] == top_key, \"sender\"].value_counts().idxmax()\n",
    "        return str(chosen), \"inferred_chat_coverage\"\n",
    "\n",
    "    if cfg_raw and cfg_key not in MY_NAME_PLACEHOLDERS:\n",
    "        return cfg_raw, \"config_no_match\"\n",
    "\n",
    "    chosen = tmp[\"sender\"].value_counts().idxmax()\n",
    "    return str(chosen), \"inferred_top_count\"\n",
    "\n",
    "\n",
    "messages = messages.copy()\n",
    "messages[\"sender\"] = messages[\"sender\"].astype(str).map(_normalize_sender_name)\n",
    "messages[\"sender_key\"] = messages[\"sender\"].map(_sender_key)\n",
    "\n",
    "configured_name = str(MY_NAME or \"\").strip()\n",
    "MY_NAME, my_name_source = _resolve_my_name(messages, configured_name)\n",
    "my_name_key = _sender_key(MY_NAME)\n",
    "messages[\"is_me\"] = messages[\"sender_key\"] == my_name_key if my_name_key else False\n",
    "\n",
    "print(f\"MY_NAME configurado={repr(configured_name)} | efectivo={repr(MY_NAME)} | source={my_name_source}\")\n",
    "print(f\"Mensajes propios detectados: {int(messages['is_me'].sum())} / {len(messages)}\")\n",
    "\n",
    "messages[\"text_raw\"] = messages[\"text\"].astype(str)\n",
    "messages[\"text\"] = messages[\"text_raw\"].apply(_normalize_text)\n",
    "messages = messages[messages[\"text\"].str.len() > 0]\n",
    "messages[\"has_url\"] = messages[\"text\"].apply(_has_url)\n",
    "messages[\"is_question\"] = messages[\"text\"].apply(_is_question)\n",
    "messages[\"is_low_signal\"] = messages[\"text\"].apply(_is_low_signal)\n",
    "messages[\"signal_score\"] = messages.apply(lambda r: _signal_score(r[\"text\"], r[\"is_question\"], r[\"has_url\"]), axis=1)\n",
    "messages[\"token_len\"] = messages[\"text\"].str.split().str.len()\n",
    "messages[\"self_fact\"] = messages[\"is_me\"] & messages[\"text\"].apply(_is_self_fact)\n",
    "\n",
    "before = len(messages)\n",
    "messages = messages.drop_duplicates(subset=[\"chat_name\", \"timestamp\", \"sender\", \"text\"])\n",
    "print(f\"Limpieza lista. Filas: {before} -> {len(messages)}\")\n",
    "print(messages[[\"sender\", \"text\", \"is_question\", \"is_low_signal\", \"signal_score\"]].head(3))\n",
    "\n",
    "try:\n",
    "    messages.to_parquet(DATA_DIR / \"messages_clean.parquet\", index=False)\n",
    "    print(f\"Guardado messages_clean.parquet con {len(messages)} filas en {DATA_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ No se pudo escribir parquet ({e}) -> guardando CSV\")\n",
    "    messages.to_csv(DATA_DIR / \"messages_clean.csv\", index=False)\n",
    "    print(f\"Guardado messages_clean.csv con {len(messages)} filas en {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6c363",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Construir memoria por eventos (no solo Q/A)\n",
    "Se crean tres tipos de unidades para recuperar mejor evidencia en WhatsApp real:\n",
    "- `qa_turn`: contexto previo -> respuesta tuya.\n",
    "- `my_message`: mensajes tuyos individuales (con micro-contexto).\n",
    "- `topic_block`: bloques de 3 a 8 mensajes dentro de un mismo tema (segmentado por pausas de tiempo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "QA_MAX_GAP_MIN = 30\n",
    "QA_REQUIRE_OTHER = True\n",
    "QA_REQUIRE_QUESTION = False\n",
    "BLOCK_MIN_SIGNAL = 0.15\n",
    "MEMORY_BUILD_VERSION = \"v10_dual_rag_units_is_me\"\n",
    "MEMORY_BUILD_MANIFEST = DATA_DIR / \"memory_build_manifest.json\"\n",
    "MEMORY_UNITS_PATH = DATA_DIR / \"memory_units.parquet\"\n",
    "\n",
    "# Datasets derivados para RAG dual\n",
    "DUAL_UNITS_MANIFEST = DATA_DIR / \"dual_units_manifest.json\"\n",
    "RESPONSE_UNITS_PATH = DATA_DIR / \"response_units.parquet\"\n",
    "STYLE_UNITS_PATH = DATA_DIR / \"style_units.parquet\"\n",
    "\n",
    "# Control de coste del paso 5\n",
    "BLOCK_STRIDE_SMALL = 2\n",
    "BLOCK_STRIDE_MEDIUM = 6\n",
    "BLOCK_STRIDE_LARGE = 12\n",
    "MAX_BLOCKS_PER_TOPIC = 120\n",
    "MAX_BLOCKS_TOTAL = 250000\n",
    "LOG_EVERY_CHATS = 10\n",
    "\n",
    "# Filtros de calidad\n",
    "MIN_RESPONSE_CHARS = 2\n",
    "MIN_STYLE_CHARS = 2\n",
    "MAX_STYLE_WORDS = 60\n",
    "\n",
    "\n",
    "def _quick_messages_fingerprint(df: pd.DataFrame):\n",
    "    cols = [\"timestamp\", \"sender\", \"text\", \"chat_name\"]\n",
    "    base = df[cols].copy()\n",
    "    base[\"timestamp\"] = pd.to_datetime(base[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    n = len(base)\n",
    "    if n == 0:\n",
    "        return \"empty\"\n",
    "\n",
    "    ts_min = str(base[\"timestamp\"].min())\n",
    "    ts_max = str(base[\"timestamp\"].max())\n",
    "\n",
    "    # Muestra estable y acotada: evita ordenar todo el dataset.\n",
    "    step = max(1, n // 5000)\n",
    "    sample = base.iloc[::step].head(5000).fillna(\"\")\n",
    "    sample_hash = pd.util.hash_pandas_object(sample, index=False)\n",
    "\n",
    "    h = hashlib.sha256()\n",
    "    h.update(f\"{n}|{ts_min}|{ts_max}|{step}\".encode(\"utf-8\"))\n",
    "    h.update(sample_hash.values.tobytes())\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def _ingest_manifest_hash():\n",
    "    p = DATA_DIR / \"ingest_cache\" / \"manifest.json\"\n",
    "    if not p.exists():\n",
    "        return \"missing\"\n",
    "    try:\n",
    "        return hashlib.sha256(p.read_bytes()).hexdigest()\n",
    "    except Exception:\n",
    "        return \"unreadable\"\n",
    "\n",
    "\n",
    "def _build_key(messages_df: pd.DataFrame):\n",
    "    payload = {\n",
    "        \"version\": MEMORY_BUILD_VERSION,\n",
    "        \"ingest_manifest_hash\": _ingest_manifest_hash(),\n",
    "        \"messages_fp\": _quick_messages_fingerprint(messages_df),\n",
    "        \"qa_max_gap_min\": QA_MAX_GAP_MIN,\n",
    "        \"qa_require_other\": QA_REQUIRE_OTHER,\n",
    "        \"qa_require_question\": QA_REQUIRE_QUESTION,\n",
    "        \"ctx_window\": CTX_WINDOW,\n",
    "        \"block_min_signal\": BLOCK_MIN_SIGNAL,\n",
    "        \"my_name\": MY_NAME,\n",
    "        \"block_stride_small\": BLOCK_STRIDE_SMALL,\n",
    "        \"block_stride_medium\": BLOCK_STRIDE_MEDIUM,\n",
    "        \"block_stride_large\": BLOCK_STRIDE_LARGE,\n",
    "        \"max_blocks_per_topic\": MAX_BLOCKS_PER_TOPIC,\n",
    "        \"max_blocks_total\": MAX_BLOCKS_TOTAL,\n",
    "        \"min_response_chars\": MIN_RESPONSE_CHARS,\n",
    "        \"min_style_chars\": MIN_STYLE_CHARS,\n",
    "        \"max_style_words\": MAX_STYLE_WORDS,\n",
    "    }\n",
    "    raw = json.dumps(payload, sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest(), payload\n",
    "\n",
    "\n",
    "def _load_json(path: Path, default):\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _save_json(path: Path, payload):\n",
    "    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "def _progress(chat_idx: int, total_chats: int):\n",
    "    return chat_idx == 1 or chat_idx % LOG_EVERY_CHATS == 0 or chat_idx == total_chats\n",
    "\n",
    "\n",
    "def _style_signature(text: str):\n",
    "    t = str(text or \"\").strip()\n",
    "    if not t:\n",
    "        return \"empty\"\n",
    "    words = t.split()\n",
    "    n_words = len(words)\n",
    "    q = int(t.endswith(\"?\"))\n",
    "    ex = int(t.endswith(\"!\"))\n",
    "    caps_ratio = sum(1 for ch in t if ch.isupper()) / max(1, sum(1 for ch in t if ch.isalpha()))\n",
    "\n",
    "    if n_words <= 2:\n",
    "        length_bucket = \"tiny\"\n",
    "    elif n_words <= 6:\n",
    "        length_bucket = \"short\"\n",
    "    elif n_words <= 14:\n",
    "        length_bucket = \"mid\"\n",
    "    else:\n",
    "        length_bucket = \"long\"\n",
    "\n",
    "    if caps_ratio > 0.35:\n",
    "        tone_bucket = \"caps\"\n",
    "    else:\n",
    "        tone_bucket = \"normal\"\n",
    "\n",
    "    return f\"{length_bucket}|q{q}|e{ex}|{tone_bucket}\"\n",
    "\n",
    "\n",
    "def build_qa_examples(df: pd.DataFrame, ctx_window: int = 4,\n",
    "                      max_gap_minutes: int = 30, require_other: bool = True, require_question: bool = False):\n",
    "    df = df.sort_values(\"timestamp\")\n",
    "    samples = []\n",
    "    total_chats = int(df[\"chat_name\"].nunique())\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    for c_idx, (chat, group) in enumerate(df.groupby(\"chat_name\"), start=1):\n",
    "        group = group.reset_index(drop=True)\n",
    "        before = len(samples)\n",
    "\n",
    "        for i, row in group.iterrows():\n",
    "            if not bool(row.get(\"is_me\", False)):\n",
    "                continue\n",
    "            if row.get(\"is_low_signal\", False):\n",
    "                continue\n",
    "\n",
    "            start = max(0, i - ctx_window)\n",
    "            ctx = group.iloc[start:i]\n",
    "            if ctx.empty:\n",
    "                continue\n",
    "\n",
    "            last_other = None\n",
    "            for r in reversed(list(ctx.itertuples())):\n",
    "                if not bool(getattr(r, \"is_me\", False)):\n",
    "                    last_other = r\n",
    "                    break\n",
    "\n",
    "            if require_other and last_other is None:\n",
    "                continue\n",
    "\n",
    "            if last_other is not None:\n",
    "                dt = row.timestamp - last_other.timestamp\n",
    "                if pd.notna(dt) and dt > timedelta(minutes=max_gap_minutes):\n",
    "                    continue\n",
    "                reply_gap_min = float(dt.total_seconds() / 60.0)\n",
    "                partner_text = str(last_other.text)\n",
    "                partner_sender = str(last_other.sender)\n",
    "            else:\n",
    "                reply_gap_min = None\n",
    "                partner_text = \"\"\n",
    "                partner_sender = \"\"\n",
    "\n",
    "            if require_question:\n",
    "                recent = ctx.tail(4)\n",
    "                if not any(getattr(r, \"is_question\", False) and not bool(getattr(r, \"is_me\", False)) for r in recent.itertuples()):\n",
    "                    continue\n",
    "\n",
    "            ctx_text = \"\\n\".join(f\"{r.sender}: {r.text}\" for r in ctx.itertuples())\n",
    "            resp = str(row.text or \"\").strip()\n",
    "            samples.append({\n",
    "                \"unit_type\": \"qa_turn\",\n",
    "                \"chat_name\": chat,\n",
    "                \"timestamp\": row.timestamp,\n",
    "                \"context\": ctx_text,\n",
    "                \"partner_text\": partner_text,\n",
    "                \"partner_sender\": partner_sender,\n",
    "                \"reply_gap_min\": reply_gap_min,\n",
    "                \"response\": resp,\n",
    "                \"style_text\": resp,\n",
    "                \"style_signature\": _style_signature(resp),\n",
    "                \"signal_score\": float(row.get(\"signal_score\", 0.5)),\n",
    "                \"self_fact\": bool(row.get(\"self_fact\", False)),\n",
    "                \"embed_text\": (\n",
    "                    f\"Ultimo mensaje del contacto:\\n{partner_text}\\n\\n\"\n",
    "                    f\"Contexto reciente:\\n{ctx_text}\\n\\n\"\n",
    "                    f\"Mi respuesta real:\\n{resp}\"\n",
    "                ),\n",
    "                \"source_id\": f\"qa::{chat}::{i}\",\n",
    "            })\n",
    "\n",
    "        if _progress(c_idx, total_chats):\n",
    "            added = len(samples) - before\n",
    "            print(f\"[Paso 5][qa] chat {c_idx}/{total_chats} | +{added} | total={len(samples)}\")\n",
    "\n",
    "    print(f\"[Paso 5][qa] fin: {len(samples)} unidades en {time.perf_counter() - t0:.1f}s\")\n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "\n",
    "def split_topics(group: pd.DataFrame, gap_minutes: int = 45):\n",
    "    group = group.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    topics = []\n",
    "    current = [0]\n",
    "    for i in range(1, len(group)):\n",
    "        dt = group.loc[i, \"timestamp\"] - group.loc[i - 1, \"timestamp\"]\n",
    "        if pd.isna(dt) or dt > timedelta(minutes=gap_minutes):\n",
    "            topics.append(group.iloc[current].copy())\n",
    "            current = [i]\n",
    "        else:\n",
    "            current.append(i)\n",
    "    if current:\n",
    "        topics.append(group.iloc[current].copy())\n",
    "    return topics\n",
    "\n",
    "\n",
    "def build_my_message_units(df: pd.DataFrame, local_window: int = 2):\n",
    "    units = []\n",
    "    total_chats = int(df[\"chat_name\"].nunique())\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    for c_idx, (chat, group) in enumerate(df.groupby(\"chat_name\"), start=1):\n",
    "        group = group.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "        before = len(units)\n",
    "\n",
    "        # Precalcula el ultimo mensaje del otro para cada fila (evita coste O(n^2)).\n",
    "        prev_other_text = [\"\"] * len(group)\n",
    "        prev_other_sender = [\"\"] * len(group)\n",
    "        last_other_text = \"\"\n",
    "        last_other_sender = \"\"\n",
    "        for i, r in enumerate(group.itertuples()):\n",
    "            prev_other_text[i] = last_other_text\n",
    "            prev_other_sender[i] = last_other_sender\n",
    "            if not bool(getattr(r, \"is_me\", False)):\n",
    "                last_other_text = str(r.text)\n",
    "                last_other_sender = str(r.sender)\n",
    "\n",
    "        for i, row in group.iterrows():\n",
    "            if not bool(row.get(\"is_me\", False)):\n",
    "                continue\n",
    "            if row.get(\"is_low_signal\", False):\n",
    "                continue\n",
    "\n",
    "            left = max(0, i - local_window)\n",
    "            right = min(len(group), i + local_window + 1)\n",
    "            local = group.iloc[left:right]\n",
    "            local_text = \"\\n\".join(f\"{r.sender}: {r.text}\" for r in local.itertuples())\n",
    "\n",
    "            partner_text = prev_other_text[i]\n",
    "            partner_sender = prev_other_sender[i]\n",
    "            resp = str(row.text or \"\").strip()\n",
    "            units.append({\n",
    "                \"unit_type\": \"my_message\",\n",
    "                \"chat_name\": chat,\n",
    "                \"timestamp\": row.timestamp,\n",
    "                \"context\": local_text,\n",
    "                \"partner_text\": partner_text,\n",
    "                \"partner_sender\": partner_sender,\n",
    "                \"reply_gap_min\": None,\n",
    "                \"response\": resp,\n",
    "                \"style_text\": resp,\n",
    "                \"style_signature\": _style_signature(resp),\n",
    "                \"signal_score\": float(row.get(\"signal_score\", 0.5)),\n",
    "                \"self_fact\": bool(row.get(\"self_fact\", False)),\n",
    "                \"embed_text\": (\n",
    "                    f\"Ultimo mensaje del contacto:\\n{partner_text}\\n\\n\"\n",
    "                    f\"Mensaje mio:\\n{resp}\\n\\n\"\n",
    "                    f\"Micro-contexto:\\n{local_text}\"\n",
    "                ),\n",
    "                \"source_id\": f\"msg::{chat}::{i}\",\n",
    "            })\n",
    "\n",
    "        if _progress(c_idx, total_chats):\n",
    "            added = len(units) - before\n",
    "            print(f\"[Paso 5][msg] chat {c_idx}/{total_chats} | +{added} | total={len(units)}\")\n",
    "\n",
    "    print(f\"[Paso 5][msg] fin: {len(units)} unidades en {time.perf_counter() - t0:.1f}s\")\n",
    "    return pd.DataFrame(units)\n",
    "\n",
    "\n",
    "def build_topic_blocks(df: pd.DataFrame, min_block: int = 3, max_block: int = 8, gap_minutes: int = 45):\n",
    "    rows = []\n",
    "    total_blocks = 0\n",
    "    total_chats = int(df[\"chat_name\"].nunique())\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    for c_idx, (chat, group) in enumerate(df.groupby(\"chat_name\"), start=1):\n",
    "        topics = split_topics(group, gap_minutes=gap_minutes)\n",
    "        topics_used = 0\n",
    "\n",
    "        for t_idx, topic in enumerate(topics):\n",
    "            topic = topic.reset_index(drop=True)\n",
    "            n = len(topic)\n",
    "            if n < min_block:\n",
    "                continue\n",
    "\n",
    "            topics_used += 1\n",
    "            if n < 200:\n",
    "                stride = BLOCK_STRIDE_SMALL\n",
    "            elif n < 2000:\n",
    "                stride = BLOCK_STRIDE_MEDIUM\n",
    "            else:\n",
    "                stride = BLOCK_STRIDE_LARGE\n",
    "\n",
    "            per_topic = 0\n",
    "            for start in range(0, n, stride):\n",
    "                for size in (min_block, 5, max_block):\n",
    "                    if size < min_block or size > max_block:\n",
    "                        continue\n",
    "                    end = start + size\n",
    "                    if end > n:\n",
    "                        continue\n",
    "\n",
    "                    block = topic.iloc[start:end]\n",
    "                    block_signal = float(block.get(\"signal_score\", pd.Series([0.5] * len(block))).mean())\n",
    "                    if block_signal < BLOCK_MIN_SIGNAL:\n",
    "                        continue\n",
    "\n",
    "                    text = \"\\n\".join(f\"{r.sender}: {r.text}\" for r in block.itertuples())\n",
    "                    rows.append({\n",
    "                        \"unit_type\": \"topic_block\",\n",
    "                        \"chat_name\": chat,\n",
    "                        \"timestamp\": block.iloc[-1].timestamp,\n",
    "                        \"context\": text,\n",
    "                        \"partner_text\": \"\",\n",
    "                        \"partner_sender\": \"\",\n",
    "                        \"reply_gap_min\": None,\n",
    "                        \"response\": \"\",\n",
    "                        \"style_text\": \"\",\n",
    "                        \"style_signature\": \"block\",\n",
    "                        \"signal_score\": block_signal,\n",
    "                        \"self_fact\": bool(block.get(\"self_fact\", pd.Series([False] * len(block))).any()),\n",
    "                        \"embed_text\": f\"Bloque de conversacion:\\n{text}\",\n",
    "                        \"source_id\": f\"blk::{chat}::{t_idx}::{start}::{end}\",\n",
    "                    })\n",
    "\n",
    "                    per_topic += 1\n",
    "                    total_blocks += 1\n",
    "                    if per_topic >= MAX_BLOCKS_PER_TOPIC:\n",
    "                        break\n",
    "                    if total_blocks >= MAX_BLOCKS_TOTAL:\n",
    "                        break\n",
    "                if per_topic >= MAX_BLOCKS_PER_TOPIC or total_blocks >= MAX_BLOCKS_TOTAL:\n",
    "                    break\n",
    "            if total_blocks >= MAX_BLOCKS_TOTAL:\n",
    "                break\n",
    "\n",
    "        if _progress(c_idx, total_chats):\n",
    "            print(\n",
    "                f\"[Paso 5][blk] chat {c_idx}/{total_chats} | \"\n",
    "                f\"topics={topics_used} | total_blocks={total_blocks}\"\n",
    "            )\n",
    "\n",
    "        if total_blocks >= MAX_BLOCKS_TOTAL:\n",
    "            print(f\"[Paso 5][blk] alcanzado MAX_BLOCKS_TOTAL={MAX_BLOCKS_TOTAL}\")\n",
    "            break\n",
    "\n",
    "    print(f\"[Paso 5][blk] fin: {len(rows)} unidades en {time.perf_counter() - t0:.1f}s\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def build_response_units(memory_units_df: pd.DataFrame):\n",
    "    base = memory_units_df.copy()\n",
    "    base = base[base[\"unit_type\"].eq(\"qa_turn\")].copy()\n",
    "    base[\"partner_text\"] = base[\"partner_text\"].fillna(\"\").astype(str).str.strip()\n",
    "    base[\"response\"] = base[\"response\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    base = base[\n",
    "        (base[\"partner_text\"].str.len() >= 1) &\n",
    "        (base[\"response\"].str.len() >= MIN_RESPONSE_CHARS)\n",
    "    ].copy()\n",
    "\n",
    "    base[\"response_embed_text\"] = (\n",
    "        \"Mensaje del contacto:\\n\" + base[\"partner_text\"] +\n",
    "        \"\\n\\nContexto:\\n\" + base[\"context\"].fillna(\"\").astype(str) +\n",
    "        \"\\n\\nMi respuesta real:\\n\" + base[\"response\"]\n",
    "    )\n",
    "    base[\"retrieval_role\"] = \"response\"\n",
    "    return base.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def build_style_units(memory_units_df: pd.DataFrame):\n",
    "    base = memory_units_df.copy()\n",
    "    base = base[base[\"unit_type\"].isin([\"qa_turn\", \"my_message\"])].copy()\n",
    "    base[\"style_text\"] = base[\"response\"].fillna(\"\").astype(str).str.strip()\n",
    "    base = base[base[\"style_text\"].str.len() >= MIN_STYLE_CHARS].copy()\n",
    "    base = base[base[\"style_text\"].str.split().str.len().clip(lower=0) <= MAX_STYLE_WORDS].copy()\n",
    "\n",
    "    base[\"style_signature\"] = base[\"style_text\"].map(_style_signature)\n",
    "    base[\"style_embed_text\"] = (\n",
    "        \"Mi mensaje:\\n\" + base[\"style_text\"] +\n",
    "        \"\\n\\nContexto breve:\\n\" + base[\"context\"].fillna(\"\").astype(str)\n",
    "    )\n",
    "    base[\"retrieval_role\"] = \"style\"\n",
    "    return base.reset_index(drop=True)\n",
    "\n",
    "\n",
    "step5_t0 = time.perf_counter()\n",
    "build_key, build_payload = _build_key(messages)\n",
    "build_manifest = _load_json(MEMORY_BUILD_MANIFEST, {})\n",
    "\n",
    "cache_hit = (\n",
    "    MEMORY_UNITS_PATH.exists() and\n",
    "    build_manifest.get(\"build_key\") == build_key\n",
    ")\n",
    "\n",
    "print(f\"[Paso 5] build_key={build_key[:12]}... | version={MEMORY_BUILD_VERSION}\")\n",
    "print(f\"[Paso 5] my_name efectivo={MY_NAME!r} | mensajes_propios={int(messages.get('is_me', pd.Series(dtype=bool)).sum())}\")\n",
    "\n",
    "if cache_hit:\n",
    "    memory_units = pd.read_parquet(MEMORY_UNITS_PATH)\n",
    "    print(\n",
    "        f\"[Paso 5] cache hit: reutilizando {len(memory_units)} unidades | \"\n",
    "        f\"updated_at={build_manifest.get('updated_at', 'n/a')}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"[Paso 5] reconstruyendo memoria (puede tardar la primera vez)...\")\n",
    "\n",
    "    t_phase = time.perf_counter()\n",
    "    qa_units = build_qa_examples(\n",
    "        messages,\n",
    "        CTX_WINDOW,\n",
    "        max_gap_minutes=QA_MAX_GAP_MIN,\n",
    "        require_other=QA_REQUIRE_OTHER,\n",
    "        require_question=QA_REQUIRE_QUESTION,\n",
    "    )\n",
    "    print(f\"[Paso 5] qa_units={len(qa_units)} | {time.perf_counter() - t_phase:.1f}s\")\n",
    "\n",
    "    t_phase = time.perf_counter()\n",
    "    my_units = build_my_message_units(messages, local_window=2)\n",
    "    print(f\"[Paso 5] my_units={len(my_units)} | {time.perf_counter() - t_phase:.1f}s\")\n",
    "    if len(qa_units) == 0 and len(my_units) == 0:\n",
    "        print(\"[Paso 5][WARN] qa_units y my_units quedaron en 0. Revisa MY_NAME en .env y la deteccion de mensajes propios.\")\n",
    "\n",
    "    t_phase = time.perf_counter()\n",
    "    block_units = build_topic_blocks(messages, min_block=3, max_block=8, gap_minutes=45)\n",
    "    print(f\"[Paso 5] block_units={len(block_units)} | {time.perf_counter() - t_phase:.1f}s\")\n",
    "\n",
    "    t_phase = time.perf_counter()\n",
    "    memory_units = pd.concat([qa_units, my_units, block_units], ignore_index=True)\n",
    "    memory_units = memory_units.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    print(f\"[Paso 5] concat+sort: {len(memory_units)} unidades | {time.perf_counter() - t_phase:.1f}s\")\n",
    "\n",
    "    memory_units.to_parquet(MEMORY_UNITS_PATH, index=False)\n",
    "\n",
    "    build_manifest = {\n",
    "        \"build_key\": build_key,\n",
    "        \"payload\": build_payload,\n",
    "        \"rows\": int(len(memory_units)),\n",
    "        \"updated_at\": pd.Timestamp.now(\"UTC\").isoformat(),\n",
    "    }\n",
    "    _save_json(MEMORY_BUILD_MANIFEST, build_manifest)\n",
    "    print(f\"[Paso 5] reconstruido: {len(memory_units)} unidades\")\n",
    "\n",
    "# Construccion/cache de unidades duales (response/style)\n",
    "dual_manifest = _load_json(DUAL_UNITS_MANIFEST, {})\n",
    "dual_hit = (\n",
    "    RESPONSE_UNITS_PATH.exists() and\n",
    "    STYLE_UNITS_PATH.exists() and\n",
    "    dual_manifest.get(\"build_key\") == build_key\n",
    ")\n",
    "\n",
    "if dual_hit:\n",
    "    response_units = pd.read_parquet(RESPONSE_UNITS_PATH)\n",
    "    style_units = pd.read_parquet(STYLE_UNITS_PATH)\n",
    "    print(\n",
    "        f\"[Paso 5] dual cache hit: response={len(response_units)} | style={len(style_units)}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"[Paso 5] construyendo datasets duales (response/style)...\")\n",
    "    t_phase = time.perf_counter()\n",
    "    response_units = build_response_units(memory_units)\n",
    "    style_units = build_style_units(memory_units)\n",
    "\n",
    "    response_units.to_parquet(RESPONSE_UNITS_PATH, index=False)\n",
    "    style_units.to_parquet(STYLE_UNITS_PATH, index=False)\n",
    "\n",
    "    dual_manifest = {\n",
    "        \"build_key\": build_key,\n",
    "        \"rows_response\": int(len(response_units)),\n",
    "        \"rows_style\": int(len(style_units)),\n",
    "        \"updated_at\": pd.Timestamp.now(\"UTC\").isoformat(),\n",
    "    }\n",
    "    _save_json(DUAL_UNITS_MANIFEST, dual_manifest)\n",
    "    print(\n",
    "        f\"[Paso 5] dual reconstruido en {time.perf_counter() - t_phase:.1f}s | \"\n",
    "        f\"response={len(response_units)} | style={len(style_units)}\"\n",
    "    )\n",
    "\n",
    "print(memory_units[\"unit_type\"].value_counts())\n",
    "if not style_units.empty:\n",
    "    print(\"[Paso 5] style_signature top:\")\n",
    "    print(style_units[\"style_signature\"].value_counts().head(10))\n",
    "print(memory_units.head(3))\n",
    "print(f\"[Paso 5] Guardadas {len(memory_units)} unidades en {DATA_DIR} | tiempo total {time.perf_counter() - step5_t0:.1f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f564633",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Embeddings locales + FAISS (indice dual por eventos)\n",
    "Se indexan unidades heterogeneas (`qa_turn`, `my_message`, `topic_block`) con cache incremental por hash.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a418ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import math\n",
    "\n",
    "if memory_units.empty:\n",
    "    raise SystemExit(\"No hay unidades: revisa parseo y construccion de memoria\")\n",
    "\n",
    "response_units_path = DATA_DIR / \"response_units.parquet\"\n",
    "style_units_path = DATA_DIR / \"style_units.parquet\"\n",
    "\n",
    "if response_units_path.exists() and style_units_path.exists():\n",
    "    response_units = pd.read_parquet(response_units_path)\n",
    "    style_units = pd.read_parquet(style_units_path)\n",
    "else:\n",
    "    raise SystemExit(\"Faltan response_units/style_units del paso 5. Ejecuta paso 5 completo.\")\n",
    "\n",
    "if response_units.empty or style_units.empty:\n",
    "    raise SystemExit(\"response_units o style_units esta vacio. Revisa limpieza y parseo.\")\n",
    "\n",
    "response_index_path = INDEX_DIR / \"response.index\"\n",
    "style_index_path = INDEX_DIR / \"style.index\"\n",
    "response_meta_path = INDEX_DIR / \"response_units.parquet\"\n",
    "style_meta_path = INDEX_DIR / \"style_units.parquet\"\n",
    "response_emb_path = INDEX_DIR / \"response_embeddings.npy\"\n",
    "style_emb_path = INDEX_DIR / \"style_embeddings.npy\"\n",
    "style_cluster_manifest_path = INDEX_DIR / \"style_cluster_manifest.json\"\n",
    "\n",
    "EMBED_INPUT_VERSION_RESPONSE = \"v1_response_dual\"\n",
    "EMBED_INPUT_VERSION_STYLE = \"v1_style_dual\"\n",
    "STYLE_CLUSTER_VERSION = \"v1_faiss_kmeans\"\n",
    "\n",
    "\n",
    "def _row_hash(row, text_col: str, version: str):\n",
    "    payload = (\n",
    "        f\"{version}\\n<SEP>\\n\"\n",
    "        f\"{row.get('source_id', '')}\\n<SEP>\\n\"\n",
    "        f\"{row.get('chat_name', '')}\\n<SEP>\\n\"\n",
    "        f\"{row.get('timestamp', '')}\\n<SEP>\\n\"\n",
    "        f\"{row.get(text_col, '')}\"\n",
    "    )\n",
    "    return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def _ensure_embedder():\n",
    "    global embedder\n",
    "    if \"embedder\" not in globals() or embedder is None:\n",
    "        embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "\n",
    "def _build_embeddings(units_df: pd.DataFrame, text_col: str, hash_col: str,\n",
    "                      meta_path: Path, emb_path: Path, version: str, label: str):\n",
    "    work = units_df.copy()\n",
    "    work[hash_col] = work.apply(lambda r: _row_hash(r, text_col=text_col, version=version), axis=1)\n",
    "\n",
    "    cached_map = {}\n",
    "    if meta_path.exists() and emb_path.exists():\n",
    "        cached_units = pd.read_parquet(meta_path)\n",
    "        cached_emb = np.load(emb_path)\n",
    "        if hash_col not in cached_units.columns:\n",
    "            cached_units[hash_col] = cached_units.apply(lambda r: _row_hash(r, text_col=text_col, version=version), axis=1)\n",
    "        for h, emb in zip(cached_units[hash_col].tolist(), cached_emb):\n",
    "            cached_map[h] = emb\n",
    "\n",
    "    hashes = work[hash_col].tolist()\n",
    "    missing = [h for h in hashes if h not in cached_map]\n",
    "\n",
    "    if missing:\n",
    "        print(f\"[Paso 6][{label}] embeddings nuevos/cambiados: {len(missing)}\")\n",
    "        _ensure_embedder()\n",
    "        mask_new = work[hash_col].isin(missing)\n",
    "        texts = work.loc[mask_new, text_col].fillna(\"\").astype(str).tolist()\n",
    "        new_emb = embedder.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True,\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        for h, emb in zip(work.loc[mask_new, hash_col].tolist(), new_emb):\n",
    "            cached_map[h] = emb\n",
    "    else:\n",
    "        print(f\"[Paso 6][{label}] sin cambios: reutilizando embeddings\")\n",
    "        _ensure_embedder()\n",
    "\n",
    "    emb_matrix = np.stack([cached_map[h] for h in hashes]).astype(\"float32\")\n",
    "    return work, emb_matrix\n",
    "\n",
    "\n",
    "def _build_faiss_index(embeddings: np.ndarray):\n",
    "    idx = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    idx.add(embeddings)\n",
    "    return idx\n",
    "\n",
    "\n",
    "# 1) Response index\n",
    "response_units_work, response_embeddings = _build_embeddings(\n",
    "    response_units,\n",
    "    text_col=\"response_embed_text\",\n",
    "    hash_col=\"response_unit_hash\",\n",
    "    meta_path=response_meta_path,\n",
    "    emb_path=response_emb_path,\n",
    "    version=EMBED_INPUT_VERSION_RESPONSE,\n",
    "    label=\"response\",\n",
    ")\n",
    "\n",
    "response_index = _build_faiss_index(response_embeddings)\n",
    "faiss.write_index(response_index, str(response_index_path))\n",
    "response_units_work.to_parquet(response_meta_path, index=False)\n",
    "np.save(response_emb_path, response_embeddings)\n",
    "print(f\"[Paso 6] response index listo: {len(response_units_work)} unidades\")\n",
    "\n",
    "\n",
    "# 2) Style index\n",
    "style_units_work, style_embeddings = _build_embeddings(\n",
    "    style_units,\n",
    "    text_col=\"style_embed_text\",\n",
    "    hash_col=\"style_unit_hash\",\n",
    "    meta_path=style_meta_path,\n",
    "    emb_path=style_emb_path,\n",
    "    version=EMBED_INPUT_VERSION_STYLE,\n",
    "    label=\"style\",\n",
    ")\n",
    "\n",
    "style_index = _build_faiss_index(style_embeddings)\n",
    "faiss.write_index(style_index, str(style_index_path))\n",
    "\n",
    "\n",
    "# 3) Categorizacion automatica de estilo (clusters semanticos)\n",
    "def _style_cluster_key(units_df: pd.DataFrame):\n",
    "    raw = \"\\n\".join(units_df[\"style_unit_hash\"].astype(str).tolist())\n",
    "    payload = f\"{STYLE_CLUSTER_VERSION}\\n{raw}\"\n",
    "    return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def _load_json(path: Path, default):\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def _save_json(path: Path, payload):\n",
    "    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "cluster_key = _style_cluster_key(style_units_work)\n",
    "cluster_manifest = _load_json(style_cluster_manifest_path, {})\n",
    "cluster_hit = (\n",
    "    \"style_cluster\" in style_units_work.columns and\n",
    "    cluster_manifest.get(\"cluster_key\") == cluster_key\n",
    ")\n",
    "\n",
    "if cluster_hit:\n",
    "    print(\"[Paso 6][style] clusters reutilizados\")\n",
    "else:\n",
    "    n = len(style_embeddings)\n",
    "    if n < 30:\n",
    "        style_units_work[\"style_cluster\"] = 0\n",
    "        print(\"[Paso 6][style] pocos datos: cluster unico\")\n",
    "    else:\n",
    "        k = min(24, max(6, int(math.sqrt(n / 180.0))))\n",
    "        train_size = min(n, 50000)\n",
    "        if train_size < n:\n",
    "            rng = np.random.default_rng(42)\n",
    "            sel = rng.choice(n, size=train_size, replace=False)\n",
    "            train_x = style_embeddings[sel]\n",
    "        else:\n",
    "            train_x = style_embeddings\n",
    "\n",
    "        kmeans = faiss.Kmeans(\n",
    "            d=style_embeddings.shape[1],\n",
    "            k=k,\n",
    "            niter=20,\n",
    "            verbose=False,\n",
    "            seed=42,\n",
    "            gpu=False,\n",
    "        )\n",
    "        kmeans.train(train_x.astype(\"float32\"))\n",
    "        _, labels = kmeans.index.search(style_embeddings.astype(\"float32\"), 1)\n",
    "        style_units_work[\"style_cluster\"] = labels.reshape(-1).astype(int)\n",
    "        print(f\"[Paso 6][style] clusters generados: k={k}\")\n",
    "\n",
    "    cluster_manifest = {\n",
    "        \"cluster_key\": cluster_key,\n",
    "        \"rows\": int(len(style_units_work)),\n",
    "        \"updated_at\": pd.Timestamp.now(\"UTC\").isoformat(),\n",
    "    }\n",
    "    _save_json(style_cluster_manifest_path, cluster_manifest)\n",
    "\n",
    "style_units_work.to_parquet(style_meta_path, index=False)\n",
    "np.save(style_emb_path, style_embeddings)\n",
    "\n",
    "print(f\"[Paso 6] style index listo: {len(style_units_work)} unidades\")\n",
    "print(\"[Paso 6] top clusters de estilo:\")\n",
    "print(style_units_work[\"style_cluster\"].value_counts().head(10))\n",
    "print(f\"Indices duales guardados en {INDEX_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44fb87",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Recuperación y chat (OpenAI API + memoria híbrida)\n",
    "La recuperación combina evidencia de mensajes individuales y bloques de conversación.\n",
    "Si la evidencia está fragmentada, se une; si no hay soporte, responde con cautela.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80aa9a",
   "metadata": {},
   "source": [
    "### Configurar OpenAI desde `.env`\n",
    "El notebook carga `../.env` automáticamente en la sección de configuración.\n",
    "Crea ese archivo a partir de `../.env.example` y define `OPENAI_API_KEY` (y opcionalmente `GEN_MODEL`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eccb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "load_dotenv(PROJECT_ROOT / \".env\", override=True)\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\", \"\"):\n",
    "    print(\"OPENAI_API_KEY detectado en ../.env\")\n",
    "else:\n",
    "    print(\"Falta OPENAI_API_KEY en ../.env\")\n",
    "print(\"GEN_MODEL:\", os.getenv(\"GEN_MODEL\", \"gpt-4.1-mini\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6608102c",
   "metadata": {},
   "source": [
    "\n",
    "### Comprobar credenciales de OpenAI\n",
    "La celda siguiente valida si `OPENAI_API_KEY` está definido en el entorno del kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2427a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verificar credenciales OpenAI\n",
    "import os\n",
    "key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if key:\n",
    "    print(\"OPENAI_API_KEY detectado\")\n",
    "else:\n",
    "    print(\"Falta OPENAI_API_KEY en el entorno del kernel\")\n",
    "print(\"GEN_MODEL:\", os.getenv(\"GEN_MODEL\", \"gpt-4.1-mini\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not api_key:\n",
    "    raise SystemExit(\"Falta OPENAI_API_KEY: define la variable en ../.env\")\n",
    "\n",
    "GEN_MODEL = os.getenv(\"GEN_MODEL\", \"gpt-4.1-mini\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Eres la persona que habla en los mensajes de WhatsApp. Respondes en primera persona y en espanol. \"\n",
    "    \"Debes generar una respuesta nueva, no copiar literalmente ejemplos. \"\n",
    "    \"Usa la evidencia de respuestas para decidir contenido y la evidencia de estilo para el tono. \"\n",
    "    \"Si no hay soporte suficiente para un dato, dilo con honestidad.\"\n",
    ")\n",
    "\n",
    "response_units_cached = pd.read_parquet(INDEX_DIR / \"response_units.parquet\")\n",
    "style_units_cached = pd.read_parquet(INDEX_DIR / \"style_units.parquet\")\n",
    "response_index_cached = faiss.read_index(str(INDEX_DIR / \"response.index\"))\n",
    "style_index_cached = faiss.read_index(str(INDEX_DIR / \"style.index\"))\n",
    "response_embeddings_cached = np.load(INDEX_DIR / \"response_embeddings.npy\")\n",
    "style_embeddings_cached = np.load(INDEX_DIR / \"style_embeddings.npy\")\n",
    "\n",
    "if \"embedder\" not in globals() or embedder is None:\n",
    "    embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "max_ts_response = pd.to_datetime(response_units_cached[\"timestamp\"], errors=\"coerce\").max()\n",
    "max_ts_style = pd.to_datetime(style_units_cached[\"timestamp\"], errors=\"coerce\").max()\n",
    "\n",
    "\n",
    "def _chat_create(messages, temperature=None):\n",
    "    kwargs = {\n",
    "        \"model\": GEN_MODEL,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "    if temperature is not None:\n",
    "        kwargs[\"temperature\"] = temperature\n",
    "    try:\n",
    "        return client.chat.completions.create(**kwargs)\n",
    "    except Exception as e:\n",
    "        msg = str(e).lower()\n",
    "        if \"temperature\" in msg and (\"unsupported\" in msg or \"only the default\" in msg):\n",
    "            return client.chat.completions.create(model=GEN_MODEL, messages=messages)\n",
    "        raise\n",
    "\n",
    "\n",
    "def _contact_hint(query: str):\n",
    "    q = query.lower()\n",
    "    all_chats = pd.concat(\n",
    "        [response_units_cached[\"chat_name\"], style_units_cached[\"chat_name\"]],\n",
    "        ignore_index=True,\n",
    "    ).dropna().unique().tolist()\n",
    "    for chat in all_chats:\n",
    "        low = str(chat).lower()\n",
    "        if low in q:\n",
    "            return chat\n",
    "    return None\n",
    "\n",
    "\n",
    "def _recency_boost(ts, max_ts, alpha=0.05):\n",
    "    ts = pd.to_datetime(ts, errors=\"coerce\")\n",
    "    if pd.isna(ts) or pd.isna(max_ts):\n",
    "        return 0.0\n",
    "    days = max((max_ts - ts).days, 0)\n",
    "    return max(0.0, alpha - days * 0.001)\n",
    "\n",
    "\n",
    "def _norm_text(s: str):\n",
    "    s = (s or \"\").lower().strip()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"[¿?¡!.,;:\\-_\\\"'()\\[\\]]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def _retrieve_response_hits(query: str, k: int = 6, min_score: float = 0.22, contact_hint: str = None):\n",
    "    q_vec = embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True).astype(\"float32\")\n",
    "    scores, idxs = response_index_cached.search(q_vec, max(60, k * 8))\n",
    "\n",
    "    out = []\n",
    "    for score, idx in zip(scores[0], idxs[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        s = float(score)\n",
    "        if s < min_score:\n",
    "            continue\n",
    "\n",
    "        row = response_units_cached.iloc[idx].to_dict()\n",
    "        signal = float(row.get(\"signal_score\", 0.4) or 0.4)\n",
    "        recency = _recency_boost(row.get(\"timestamp\"), max_ts_response, alpha=0.05)\n",
    "        contact_boost = 0.04 if (contact_hint and row.get(\"chat_name\") == contact_hint) else 0.0\n",
    "\n",
    "        gap = row.get(\"reply_gap_min\")\n",
    "        if gap is None or pd.isna(gap):\n",
    "            gap_bonus = 0.0\n",
    "        else:\n",
    "            gap = float(gap)\n",
    "            gap_bonus = max(0.0, 0.03 - min(gap, 120.0) * 0.00025)\n",
    "\n",
    "        row[\"score\"] = s\n",
    "        row[\"rank_score\"] = s + 0.08 * signal + recency + contact_boost + gap_bonus\n",
    "        row[\"retrieval_role\"] = \"response\"\n",
    "        out.append(row)\n",
    "\n",
    "    out = sorted(out, key=lambda x: x[\"rank_score\"], reverse=True)\n",
    "    dedup = []\n",
    "    seen = set()\n",
    "    for r in out:\n",
    "        key = r.get(\"source_id\")\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        dedup.append(r)\n",
    "        if len(dedup) >= k:\n",
    "            break\n",
    "    return dedup\n",
    "\n",
    "\n",
    "def _retrieve_style_hits(query: str, k: int = 10, min_score: float = 0.20, contact_hint: str = None):\n",
    "    q_vec = embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True).astype(\"float32\")\n",
    "    scores, idxs = style_index_cached.search(q_vec, max(80, k * 8))\n",
    "\n",
    "    ranked = []\n",
    "    for score, idx in zip(scores[0], idxs[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        s = float(score)\n",
    "        if s < min_score:\n",
    "            continue\n",
    "\n",
    "        row = style_units_cached.iloc[idx].to_dict()\n",
    "        signal = float(row.get(\"signal_score\", 0.4) or 0.4)\n",
    "        recency = _recency_boost(row.get(\"timestamp\"), max_ts_style, alpha=0.04)\n",
    "        contact_boost = 0.03 if (contact_hint and row.get(\"chat_name\") == contact_hint) else 0.0\n",
    "\n",
    "        text = str(row.get(\"style_text\", \"\") or \"\").strip()\n",
    "        n_words = len(text.split())\n",
    "        len_bonus = min(0.04, n_words / 400.0)\n",
    "\n",
    "        row[\"score\"] = s\n",
    "        row[\"rank_score\"] = s + 0.07 * signal + recency + contact_boost + len_bonus\n",
    "        row[\"retrieval_role\"] = \"style\"\n",
    "        ranked.append(row)\n",
    "\n",
    "    ranked = sorted(ranked, key=lambda x: x[\"rank_score\"], reverse=True)\n",
    "\n",
    "    # Diversificacion por cluster: evita repetir siempre el mismo patron de respuesta.\n",
    "    buckets = {}\n",
    "    for r in ranked:\n",
    "        cluster = int(r.get(\"style_cluster\", -1))\n",
    "        buckets.setdefault(cluster, []).append(r)\n",
    "\n",
    "    diversified = []\n",
    "    seen_text = set()\n",
    "    clusters = list(buckets.keys())\n",
    "    cursor = {c: 0 for c in clusters}\n",
    "\n",
    "    while len(diversified) < k and clusters:\n",
    "        next_clusters = []\n",
    "        for c in clusters:\n",
    "            i = cursor[c]\n",
    "            arr = buckets[c]\n",
    "            while i < len(arr):\n",
    "                cand = arr[i]\n",
    "                i += 1\n",
    "                t_key = _norm_text(str(cand.get(\"style_text\", \"\")))\n",
    "                if not t_key or t_key in seen_text:\n",
    "                    continue\n",
    "                seen_text.add(t_key)\n",
    "                diversified.append(cand)\n",
    "                break\n",
    "            cursor[c] = i\n",
    "            if i < len(arr):\n",
    "                next_clusters.append(c)\n",
    "            if len(diversified) >= k:\n",
    "                break\n",
    "        clusters = next_clusters\n",
    "\n",
    "    return diversified\n",
    "\n",
    "\n",
    "def retrieve_bundle(query: str, k_response: int = 6, k_style: int = 8, min_score: float = 0.22):\n",
    "    contact = _contact_hint(query)\n",
    "    response_hits = _retrieve_response_hits(query, k=k_response, min_score=min_score, contact_hint=contact)\n",
    "    style_hits = _retrieve_style_hits(query, k=k_style, min_score=min_score * 0.9, contact_hint=contact)\n",
    "    return {\n",
    "        \"response_hits\": response_hits,\n",
    "        \"style_hits\": style_hits,\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve(query: str, k_total: int = 10, min_score: float = 0.22):\n",
    "    bundle = retrieve_bundle(\n",
    "        query,\n",
    "        k_response=max(4, k_total // 2),\n",
    "        k_style=max(4, k_total),\n",
    "        min_score=min_score,\n",
    "    )\n",
    "    merged = []\n",
    "    merged.extend(bundle[\"response_hits\"])\n",
    "    merged.extend(bundle[\"style_hits\"])\n",
    "    merged = sorted(merged, key=lambda x: x.get(\"rank_score\", x.get(\"score\", 0.0)), reverse=True)\n",
    "    return merged[:k_total]\n",
    "\n",
    "\n",
    "def answer(prompt: str, k_total: int = 10, temperature: float = None, min_score: float = 0.22):\n",
    "    k_response = max(4, k_total // 2)\n",
    "    k_style = max(6, k_total)\n",
    "    bundle = retrieve_bundle(prompt, k_response=k_response, k_style=k_style, min_score=min_score)\n",
    "\n",
    "    response_hits = bundle[\"response_hits\"]\n",
    "    style_hits = bundle[\"style_hits\"]\n",
    "\n",
    "    if not response_hits and not style_hits:\n",
    "        return \"No tengo evidencia suficiente en mis chats para responder eso con seguridad.\"\n",
    "\n",
    "    response_block = \"\\n\\n\".join(\n",
    "        (\n",
    "            f\"[response | score {h['score']:.3f} | chat: {h['chat_name']} | fecha: {h['timestamp']}]\\n\"\n",
    "            f\"Mensaje del contacto: {h.get('partner_text', '')}\\n\"\n",
    "            f\"Mi respuesta real: {h.get('response', '')}\\n\"\n",
    "            f\"Contexto: {h.get('context', '')}\"\n",
    "        )\n",
    "        for h in response_hits\n",
    "    )\n",
    "\n",
    "    style_block = \"\\n\".join(\n",
    "        f\"- ({h.get('style_cluster', -1)}) {h.get('style_text', '')}\"\n",
    "        for h in style_hits[:8]\n",
    "    )\n",
    "\n",
    "    user_msg = (\n",
    "        \"Consulta nueva:\\n\" + prompt +\n",
    "        \"\\n\\nEvidencia de como respondo a mensajes parecidos:\\n\" + (response_block or \"(sin evidencia de respuesta)\") +\n",
    "        \"\\n\\nMuestras de mi estilo de escritura:\\n\" + (style_block or \"(sin muestras de estilo)\") +\n",
    "        \"\\n\\nInstrucciones de salida:\\n\"\n",
    "        \"1) Genera una sola respuesta como la escribiria yo.\\n\"\n",
    "        \"2) Usa la evidencia de respuesta para el contenido.\\n\"\n",
    "        \"3) Usa las muestras de estilo para tono/forma, sin copiar literal.\\n\"\n",
    "        \"4) Si falta evidencia para afirmar algo, dilo con claridad.\"\n",
    "    )\n",
    "\n",
    "    completion = _chat_create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    text = (completion.choices[0].message.content or \"\").strip()\n",
    "\n",
    "    if not text:\n",
    "        return \"No tengo evidencia suficiente en mis chats para responder eso con seguridad.\"\n",
    "\n",
    "    # Segundo intento ligero solo si devuelve eco exacto.\n",
    "    if _norm_text(text) == _norm_text(prompt):\n",
    "        retry = _chat_create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_msg + \"\\n\\nReformula sin repetir la consulta.\"},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        text2 = (retry.choices[0].message.content or \"\").strip()\n",
    "        if text2:\n",
    "            text = text2\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced18212",
   "metadata": {},
   "source": [
    "\n",
    "### Probar recuperación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieve(\"¿Que tal?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5128ab",
   "metadata": {},
   "source": [
    "\n",
    "### Probar chat (OpenAI API)\n",
    "Si `OPENAI_API_KEY` y `GEN_MODEL` están definidos en `../.env`, ejecuta la celda de abajo con `answer(...)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5809a8",
   "metadata": {},
   "source": [
    "\n",
    "### Nota de costes y privacidad\n",
    "En modo API, cada consulta envía contexto recuperado al proveedor y consume tokens facturables.\n",
    "Ajusta `k` en `answer(prompt, k=...)` para controlar coste/contexto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15808a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(\"Te gusta la carbonara?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
